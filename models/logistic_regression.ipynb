{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: max_features=40000, C=10\n"
     ]
    }
   ],
   "source": [
    "# load best hyperparameters\n",
    "with open('../results/logistic_regression_tuning_best.json', 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_max_features = best_params['max_features']\n",
    "best_c = best_params['C']\n",
    "\n",
    "print(f\"Best hyperparameters: max_features={best_max_features}, C={best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (159571, 8)\n",
      "Test data: (63978, 8)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test_1.csv')\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (159571, 40000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(max_features=best_max_features, ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train['comment_text'])\n",
    "X_test = vectorizer.transform(test['comment_text'])\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final logistic regression models with C=10, max_features=40000\n",
      "\n",
      "Training model for toxic...\n",
      "Training model for severe_toxic...\n",
      "Training model for obscene...\n",
      "Training model for threat...\n",
      "Training model for insult...\n",
      "Training model for identity_hate...\n",
      "\n",
      "All models trained!\n"
     ]
    }
   ],
   "source": [
    "# train final models\n",
    "print(f\"Training final logistic regression models with C={best_c}, max_features={best_max_features}\\n\")\n",
    "\n",
    "# models\n",
    "models = {}\n",
    "\n",
    "# train one model for each label\n",
    "for label in labels:\n",
    "    print(f\"Training model for {label}...\")\n",
    "    \n",
    "    y_train = train[label]\n",
    "    \n",
    "    model = LogisticRegression(C=best_c, penalty='l2', class_weight='balanced', max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    models[label] = model\n",
    "\n",
    "print(\"\\nAll models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST SET RESULTS\n",
      "==================================================\n",
      "Macro Precision: 0.347\n",
      "Macro Recall:    0.820\n",
      "Macro F1:        0.477\n",
      "Macro AUC-PR:    0.576\n",
      "\n",
      "Per-label metrics:\n",
      "Label           Precision    Recall       F1           AUC-PR      \n",
      "---------------------------------------------------------------\n",
      "toxic           0.454        0.891        0.602        0.755       \n",
      "severe_toxic    0.171        0.817        0.282        0.313       \n",
      "obscene         0.479        0.855        0.614        0.767       \n",
      "threat          0.253        0.777        0.382        0.422       \n",
      "insult          0.462        0.795        0.584        0.694       \n",
      "identity_hate   0.265        0.785        0.396        0.503       \n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "per_label_metrics = {}\n",
    "label_precisions = []\n",
    "label_recalls = []\n",
    "label_f1s = []\n",
    "label_auc_prs = []\n",
    "\n",
    "for label in labels:\n",
    "    # get predictions\n",
    "    model = models[label]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_scores = model.decision_function(X_test)\n",
    "    \n",
    "    # calculate per-label metrics\n",
    "    precision = precision_score(test[label], y_pred, zero_division=0)\n",
    "    recall = recall_score(test[label], y_pred, zero_division=0)\n",
    "    f1 = f1_score(test[label], y_pred, zero_division=0)\n",
    "    auc_pr = average_precision_score(test[label], y_scores)\n",
    "    \n",
    "    # store per-label metrics\n",
    "    per_label_metrics[label] = {\n",
    "        'precision': round(precision, 3),\n",
    "        'recall': round(recall, 3),\n",
    "        'f1': round(f1, 3),\n",
    "        'auc_pr': round(auc_pr, 3)\n",
    "    }\n",
    "    \n",
    "    label_precisions.append(precision)\n",
    "    label_recalls.append(recall)\n",
    "    label_f1s.append(f1)\n",
    "    label_auc_prs.append(auc_pr)\n",
    "\n",
    "# calculate macro metrics\n",
    "test_metrics = {\n",
    "    \"macro_precision\": round(np.mean(label_precisions), 3),\n",
    "    \"macro_recall\": round(np.mean(label_recalls), 3),\n",
    "    \"macro_f1\": round(np.mean(label_f1s), 3),\n",
    "    \"macro_auc_pr\": round(np.mean(label_auc_prs), 3),\n",
    "    \"per_label_metrics\": per_label_metrics\n",
    "}\n",
    "\n",
    "# display results\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Macro Precision: {test_metrics['macro_precision']:.3f}\")\n",
    "print(f\"Macro Recall:    {test_metrics['macro_recall']:.3f}\")\n",
    "print(f\"Macro F1:        {test_metrics['macro_f1']:.3f}\")\n",
    "print(f\"Macro AUC-PR:    {test_metrics['macro_auc_pr']:.3f}\")\n",
    "print(\"\\nPer-label metrics:\")\n",
    "print(f\"{'Label':<15} {'Precision':<12} {'Recall':<12} {'F1':<12} {'AUC-PR':<12}\")\n",
    "print(\"-\" * 63)\n",
    "for label in labels:\n",
    "    m = per_label_metrics[label]\n",
    "    print(f\"{label:<15} {m['precision']:<12.3f} {m['recall']:<12.3f} {m['f1']:<12.3f} {m['auc_pr']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/logistic_regression_results.json\n",
      "{\n",
      "  \"macro_precision\": 0.347,\n",
      "  \"macro_recall\": 0.82,\n",
      "  \"macro_f1\": 0.477,\n",
      "  \"macro_auc_pr\": 0.576,\n",
      "  \"per_label_auc_pr\": {\n",
      "    \"toxic\": 0.755,\n",
      "    \"severe_toxic\": 0.313,\n",
      "    \"obscene\": 0.767,\n",
      "    \"threat\": 0.422,\n",
      "    \"insult\": 0.694,\n",
      "    \"identity_hate\": 0.503\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# save final metrics\n",
    "results = {\n",
    "    \"macro_precision\": test_metrics['macro_precision'],\n",
    "    \"macro_recall\": test_metrics['macro_recall'],\n",
    "    \"macro_f1\": test_metrics['macro_f1'],\n",
    "    \"macro_auc_pr\": test_metrics['macro_auc_pr'],\n",
    "    \"per_label_auc_pr\": {label: per_label_metrics[label]['auc_pr'] for label in labels}\n",
    "}\n",
    "\n",
    "with open('../results/logistic_regression_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to ../results/logistic_regression_results.json\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
